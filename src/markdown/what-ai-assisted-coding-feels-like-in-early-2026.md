---
title: "What AI-Assisted Coding Feels Like in Early 2026"
featuredImage: "../images/tbd.jpg"
description: "Exploring what AI-assisted coding feels like as of early 2026, and the tension between speed and accountability in complex software systems."
date: "2026-02-01"
category: "productivity"
related:
  - "tbd"
  - "tbd"
  - "tbd"
---

A few weeks ago, during an engineering all-hands meeting, we were given a new mandate: no more handwritten code. Aside from quick one-liners, everything should be generated by AI, with the expectation that this would make us more productive.

Nothing about that was shocking. If anything, it felt like a natural continuation of where things have been heading.

And yet, after the meeting wrapped up, I found myself circling around a vague sense of discomfort — not disagreement, not resistance, just a quiet unease I couldn't quite put into words. And later that evening, a conversation with my husband, who is not technical, made it clear I needed to write this down.

## It's Fast — Really Fast

If you've been doing software development for a while, you've experienced the rhythm of a typical feature (details may vary based on company size, stage of project):

* Analyze the problem
* Explore the codebase
* Make design decisions (maybe you even document this, sometimes as a few paragraphs in the ticket, or maybe more formally as an ADR)
* Write the code
* Iterate and refine
* Submit for review and address reviewer's feedback
* Deploy to staging or a test environment and address business user's feedback (and/or QA if project uses human QA testers)
* Deploy to production
* Monitor/observe behaviour in production, deal with bugs if they come up

Before AI was in the picture, the act of *taking time* to think through each of those steps served a useful purpose: you marinated in the problem. That slower cadence gave you space to really absorb the requirements, grapple with edge cases, and deeply understand the business problem and code you were working on.

With AI-assisted tooling today, many parts of that process happen in minutes — minutes where you previously might have been stuck for hours. Drafts of design options, trade-off analysis, even scaffolding for new features and the entire solution can be composed almost instantly. It's remarkable!

But what hasn't sped up, and what hasn't been given much attention, is the human side of this work.

## Output vs. Understanding vs. Responsibility

I've come up with a sentence that captures a core tension I've been noticing lately:

> The volume of work AI can produce exceeds current human capacity to reason about it.

That's not a criticism of the technology, it's an observation about human cognition. There's only so much time and mental energy we have to *understand* what's been generated, evaluate whether it actually matches the business rules, find edge cases, and check that there's appropriate observability and logging for long-term support.

Sure, AI can generate code that adds logging and metrics, but the engineer responding to the pager in the middle of the night is still a human who has to decide whether those logs are actually helpful when something goes wrong. That's a kind of understanding that takes time, something the older, slower cycle of writing code naturally gave us.

In other words, AI accelerates **production of documents and code**, but it doesn't accelerate **comprehension** or **accountability** at the same rate.

## A Metaphor That Finally Works

Later that evening, I was trying to describe the situation to my husband. He doesn't work in tech, but was curious about what it feels like to have a machine do the coding humans used to do. I spent a while trying to find a metaphor that would make sense outside of software, and finally one clicked.

Imagine your usual weekly grocery shopping. Let's say it takes a couple hours between driving to the store, finding parking, picking up what you need, checking out, and driving home. You're doing it in a trusty old Honda Civic — reliable, predictable.

Now imagine that overnight, someone swaps your Civic for a **Formula 1 racing car**, and you're told you're expected to complete those same errands *10× faster* because you now have a car that is capable of insane speeds.

That sounds appealing at first, until you consider a few things about F1 cars:

* They require a *special track* to go fast
* They need *special fuel*
* They demand a *highly trained driver*
* They are supported by a whole *pit crew and telemetry infrastructure*

On a public road full of traffic lights, stop signs, pedestrians, buses, and school zones, you'd be lucky to make it out of the driveway in one piece.

In the same way, AI gives us an engine capable of ludicrous speed, but the *roads* we're actually on — mature codebases, complex requirements, regulatory or compliance constraints, legacy interactions, and real users — haven't magically turned into race tracks overnight.

We've gained horsepower. We haven't gained the infrastructure, tooling, or organization designed around moving at that speed.

## It's Not That We're Against AI

To be clear: I'm not arguing against AI-assisted coding. I've been using it almost exclusively in my work for nearly a year now, and the benefits are obvious.

This isn't a nostalgia post for handwritten code, nor a plea for slower pace. The genie isn't going back into the bottle. What I'm trying to name is a *tension* that's real in day-to-day work:

* We're being asked to produce more in less time
* We're still fully responsible for the correctness and reliability of what ships
* And the cognitive load of *truly understanding* what's been produced hasn't gotten easier

This isn't about side projects or greenfield experiments. This is about *mature systems with real users, revenue, and consequences.* It's not “move fast and break things.”

## So What Now?

I don't have a neat solution. AI has changed the pace of production, but our accountability for the systems we operate hasn't changed. That creates a tension: how do we move faster without losing our grip on understanding and responsibility?

If the speed of coding has outpaced our ability to fully absorb and stand behind it, we may need to invest in practices that help teams stay responsibly fast — for example, clearer design thinking, review processes that account for rapid generation, and organizational expectations aligned with human cognitive limits.

AI isn't going away. Neither is responsibility. The question is how we build the tools, workflows, and culture that let us keep pace safely. I'm curious what approaches other teams have tried, and what's worked (or hasn't) in practice.

## TODO
* title
* description - avoid repeating title!
* feature image
* related
* intro para
* main content
* conclusion para
- organize/order the points in "it's really fast" section
- in Metaphor section, explain about impossibility of driving at F1 speeds on city streets without injuries or property damage, and imagine you're still responsible/liable for that.
- re: "We haven't gained the infrastructure, tooling, or organization designed around moving at that speed." - somewhere work in I'm sure that will all come in good time, but not quite there now?
- telemetry and F1? https://f1briefing.com/how-metrics-shape-f1-team-outcomes/ or https://www.veritasnewspaper.org/post/telemetry-in-f1-the-invisible-communication-between-car-and-pit
* edit
