---
title: "AI Forecasts, Fear, and Focus"
featuredImage: "../images/ai-forecasts-petr-sidorov-XoBdj1zV-EA-unsplash.jpg"
description: "Why I tune out sensational AI predictions and focus on what I can actually control."
date: "2026-01-15"
category: "productivity"
related:
  - "How to Learn New Things"
  - "The Machines Finally Made Us Care About Documentation"
  - "Rapid Prototyping with ChatGPT: OAS Pension Calculator Part 1"
---

Lately, I've noticed something disturbing about my podcast feed. An increasing number of episodes seems to have some flavor of "AI will do X or Y…" predictions. From the doom‑and‑gloom end ("AI will turn humans into paperclips!") to the utopian extreme ("AI will solve all our problems!"). And when you add in variants like "AI will take all our jobs by 2030", it starts to get very anxiety inducing.

Over time I've realized: this kind of content is low‑value, and I've consciously started tuning it out. Here's why.

## Most Predictions Are Wrong

There's a body of research showing that many confident expert predictions fail to come true. One of the most widely discussed studies on this was led by Philip E. Tetlock in his book [Expert Political Judgment](https://press.princeton.edu/books/hardcover/9780691178288/expert-political-judgment) and later expanded in [Superforecasting](https://www.penguinrandomhouse.com/books/227815/superforecasting-by-philip-e-tetlock-and-dan-gardner/). Tetlock had hundreds of experts make tens of thousands of predictions about politics, economics, wars, and other complex domains, and then checked how those forecasts actually panned out.

The surprising result? The majority of so-called "experts" were often no better at predicting the future than random chance, or a "dart-throwing chimpanzee" as Tetlock put it. The most confident pundits were often among the worst at forecasting. Credentials and media prominence didn't reliably translate into accuracy.

![dart throwing chimpanzee](../images/dart-throwing-chimpanzee.png "dart throwing chimpanzee")

What did matter was how people approached prediction itself. Tetlock found that a small group he called [superforecasters](https://www.richardhughesjones.com/superforecasting-summary/) consistently performed better by thinking in probabilities rather than absolutes, breaking complex questions into smaller parts, and continuously updating their beliefs as new evidence emerged. Their edge didn't come from bold confidence or impressive credentials, but from treating uncertainty as something to be measured and revised, not ignored.

This doesn't mean all predictions are useless. But it does mean sensational, confident (and often vague) predictions are usually not trustworthy, especially when they're made for capturing attention rather than clear, measurable foresight. Which raises a question: Is there any reason to believe today's AI doomer or utopian predictions are any more accurate than the forecasts Tetlock tested?

## Negativity Bias

If confident predictions are usually unreliable, why do the most alarming ones feel so compelling anyway? The answer has to do with how our brains allocate attention.

Psychologists call this *negativity bias*: the tendency for negative information to carry more psychological weight than positive or neutral information. Threats, losses, and worst-case scenarios are processed more quickly, remembered more vividly, and exert a disproportionate influence on our judgments.

This bias isn't a modern media invention. It's a feature of human cognition shaped by evolution. For most of human history, failing to notice danger carried far higher costs than overlooking opportunity. Missing a predator could be fatal; missing a benefit usually wasn't. Our brains evolved to err on the side of vigilance.

TODO: Paul Rozin is a psychologist? scientist? what?
Modern psychology has repeatedly confirmed this pattern. In a paper titled [Negativity Bias, Negativity Dominance, and Contagion](https://www.researchgate.net/publication/228778181_Negativity_Bias_Negativity_Dominance_and_Contagion), Paul Rozin and Edward Royzman showed that negative information is more potent than equivalent positive information, tends to dominate mixed evaluations, and spreads more easily through memory and social narratives. A single negative element can outweigh many positives.

TODO: Roy Baumeister is a psychologist? scientist? what?
Roy Baumeister and colleagues extended this idea in [Bad Is Stronger than Good](https://www.researchgate.net/publication/46608952_Bad_Is_Stronger_than_Good), demonstrating that across relationships, learning, and social interaction, bad events have stronger and longer-lasting effects than good ones of similar intensity. Negative impressions form faster, are processed more deeply, and are harder to dislodge once established.

Daniel Kahneman's *Thinking, Fast and Slow* ties these findings together, explaining how related mechanisms like loss aversion and the availability heuristic cause negative possibilities to loom larger in our minds, especially under uncertainty.

In today's media environment, this ancient survival mechanism is easy to exploit. Predictions framed around catastrophe, loss, or existential threat reliably outperform cautious, probabilistic analysis in the attention economy. A podcast teaser asking *Is AI going to end civilization?* or *Will AI take your job next quarter?* triggers a very different response than one about incremental productivity gains. This type of content feels impossible to ignore because it sounds like a threat.

Your brain is doing exactly what it evolved to do: paying close attention to potential danger. The problem is that attention is being captured, not informed. Understanding negativity bias doesn't make us immune to it. But it does make the pattern easier to recognize. And once you see that many sensational AI predictions are optimized for attention rather than accuracy, tuning them out becomes less an act of denial and more an act of cognitive self-defense.

## Circles

TODO: Cleaner wording/grammar re: "tuning this stuff out..."
Part of tuning this stuff out comes from a lesson I first learned in [The 7 Habits of Highly Effective People](https://www.franklincovey.com/books/the-7-habits-of-highly-effective-people/) by Steven Covey. In Habit 1, Be Proactive, Covey introduces two concepts that are surprisingly relevant to how we engage with AI today: the Circle of Concern and the Circle of Influence.

> We each have a wide range of concerns, our health, our children, problems at work, the national debt, nuclear war… As we look at those things within our Circle of Concern, it becomes apparent that there are some things over which we have no real control and others that we can do something about. We could identify those concerns in the latter group by circumscribing them within a smaller Circle of Influence.

In other words, the Circle of Concern contains everything we care about, while the Circle of Influence contains the subset of concerns where our actions, habits, and decisions actually make a difference. Covey emphasizes that proactive people focus on the latter: they put energy into the things they can influence, expanding that circle over time. Reactive people, by contrast, spend most of their attention on worries outside their control, which can increase stress and shrink their influence.

Even though The 7 Habits of Highly Effective People was first published in 1989, this lesson is timeless. Applied to AI, much of the media hype, from doomsday scenarios to utopian promises, sits squarely in our Circle of Concern. Focusing on it might trigger anxiety or fascination, but it rarely changes anything we can act on.

By focusing instead on what we can influence, such as our skills, our understanding of AI tools, and how we use them in daily life, we can put energy into tangible progress. This mindset shift, from reactivity and speculation to acting within influence, is a simple but powerful way to regain control over our attention and mental bandwidth.

## What I Focus On

So how do I sift through the noise?

I've developed a simple personal filter:

**I tune in to content that's:**

* Educational or practical — e.g., how to use a new tool, how to solve a problem with AI in your workflow.
* Measured and specific — it gives tangible ideas instead of vague doom/utopia narratives.

**I skip content that's:**

* Entirely about "Will AI destroy everything?"
* Designed to provoke fear, outrage, or hype.
* Confidently predicting a distant future without *any clear basis in measurable reasoning*.

The result? I have more mental bandwidth to focus on what matters to me:

* learning the tools and integrating them into my daily work as an engineer,
* using AI to streamline problem‑solving,
* and even applying these tools to *everyday life* (like interpreting a medical lab results or summarizing scientific research).

That's my Circle of Influence, and it's where I believe attention, rather than fear is best spent.

## Closing Thoughts

AI is fascinating and important, and there will always be questions about where it's headed. But not all predictions are equally worth listening to. By understanding how our minds are wired, how predictions really stack up, and where our real influence lies, it's possible to cut through the hype and make your attention work for you, not against you.

## TODO
* From "predictions are wrong" to "negativity bias" question at end of one and another at beginning of next feels awkward
* intro para
  * awkward wording "it starts to feel like a constant signal drowned in noise"
* convert "What I Focus on" to more prose than bullet points maybe?
* Should the topic of superforecasters be a separate section?
  * are there any superforecasters in AI that we should be listening to?
  * is "Good Judgement Project" still active? Any AI analysis within that? https://en.wikipedia.org/wiki/The_Good_Judgment_Project
* edit
* add related after Jan. 1 published -> Slowing Down AI On Purpose
