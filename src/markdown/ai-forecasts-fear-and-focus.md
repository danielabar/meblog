---
title: "AI Forecasts, Fear, and Focus"
featuredImage: "../images/ai-forecasts-petr-sidorov-XoBdj1zV-EA-unsplash.jpg"
description: "Why I tune out sensational AI predictions and focus on what I can actually control."
date: "2026-01-15"
category: "productivity"
related:
  - "How to Learn New Things"
  - "The Machines Finally Made Us Care About Documentation"
  - "Rapid Prototyping with ChatGPT: OAS Pension Calculator Part 1"
---

Lately, I've noticed something disturbing about my podcast feed. An increasing number of episodes seems to have some flavor of "AI will do X or Y…" predictions. From the doom‑and‑gloom end ("AI will turn humans into paperclips!") to the utopian extreme ("AI will solve all our problems!"). And when you add in variants like "AI will take all our jobs by 2030", it starts to get very anxiety inducing.

Over time I've realized: this kind of content is low‑value, and I've consciously started tuning it out. Here's why.

## Most Predictions Are Wrong

There's a body of research showing that many confident expert predictions fail to come true. One of the most widely discussed studies on this was led by Philip E. Tetlock in his book [Expert Political Judgment](https://press.princeton.edu/books/hardcover/9780691178288/expert-political-judgment) and later expanded in [Superforecasting](https://www.penguinrandomhouse.com/books/227815/superforecasting-by-philip-e-tetlock-and-dan-gardner/). Tetlock had hundreds of experts make tens of thousands of predictions about politics, economics, wars, and other complex domains, and then checked how those forecasts actually panned out.

The surprising result? The majority of so-called "experts" were often no better at predicting the future than random chance, or a "dart-throwing chimpanzee" as Tetlock put it. The most confident pundits were often among the worst at forecasting. Credentials and media prominence didn't reliably translate into accuracy.

![dart throwing chimpanzee](../images/dart-throwing-chimpanzee.png "dart throwing chimpanzee")

What did matter was how people approached prediction itself. Tetlock found that a small group he called [superforecasters](https://www.richardhughesjones.com/superforecasting-summary/) consistently performed better by thinking in probabilities rather than absolutes, breaking complex questions into smaller parts, and continuously updating their beliefs as new evidence emerged. Their edge didn't come from bold confidence or impressive credentials, but from treating uncertainty as something to be measured and revised, not ignored.

This doesn't mean all predictions are useless. But it does mean sensational, confident (and often vague) predictions are usually not trustworthy, especially when they're made for capturing attention rather than clear, measurable foresight. Which raises a question: Is there any reason to believe today's AI doomer or utopian predictions are any more accurate than the forecasts Tetlock tested?

## Negativity Bias

If confident predictions are usually unreliable, why do the most alarming ones feel so compelling anyway? The answer has less to do with evidence and more to do with how our brains allocate attention.
Psychologists refer to this tendency as *negativity bias*: The finding that negative information carries more psychological weight than positive or neutral information. Threats, losses, and worst-case scenarios are processed more quickly, remembered more vividly, and exert a disproportionate influence on our judgments.

This bias wasn't discovered by AI researchers or media companies, it's a feature of human cognition shaped by evolution. For most of human history, failing to notice danger carried far higher costs than overlooking something beneficial. Missing a predator could be fatal, while missing a pleasant opportunity usually wasn't. Over time, our brains adapted to err on the side of vigilance.

Modern psychology has repeatedly confirmed this pattern. In their paper [Negativity Bias, Negativity Dominance, and Contagion](https://www.researchgate.net/publication/228778181_Negativity_Bias_Negativity_Dominance_and_Contagion), researchers Paul Rozin and Edward Royzman showed that negative information is more potent than equivalent positive information, grows more salient as it approaches in time or relevance, and tends to dominate mixed evaluations, so that a single negative element can outweigh multiple positive ones. They also highlighted that negative events are more contagious, spreading more easily across judgments, memories, and social narratives.

Roy Baumeister and colleagues extended this idea empirically in a paper titled [Bad Is Stronger than Good](https://www.researchgate.net/publication/46608952_Bad_Is_Stronger_than_Good). They demonstrated that across a wide range of domains (such as relationships, learning, and social interaction), bad events have stronger and longer-lasting effects than good ones of comparable intensity. Negative information is processed more thoroughly, forms impressions more quickly, and is harder to dislodge once established.

Daniel Kahneman’s book [Thinking, Fast and Slow](https://www.penguinrandomhouse.ca/books/89308/thinking-fast-and-slow-by-daniel-kahneman/9780385676533) offers an accessible synthesis of these findings within a broader framework of cognitive biases. This book demonstrates how related mechanisms such as loss aversion, the availability heuristic, and fast, threat-sensitive intuitive processing, cause negative possibilities to loom larger in our judgments than neutral or positive outcomes, especially under uncertainty.

In today's media environment, that ancient survival mechanism is easily exploited. Predictions framed around catastrophe, loss, or existential threat are far more likely to grab attention than cautious, probabilistic analysis. A headline asking whether AI will end civilization triggers a different neurological response than one explaining incremental productivity gains, even if the latter is far more plausible.

This is why alarming AI forecasts spread so easily. Your brain isn't responding because the prediction is well-reasoned or statistically grounded; it's responding because it sounds like a threat. The attention economy rewards whatever activates that response, regardless of predictive accuracy.

Understanding negativity bias doesn't make us immune to it, but it does make the pattern easier to recognize. And once you see that many sensational AI predictions are optimized for attention rather than truth, tuning them out becomes less an act of denial and more an act of cognitive self-defense.

TODO: Where does this sentence fit in?
So when a podcast teases "Is AI going to end civilization?" or "Will AI take your job next quarter?", your brain pays attention *because it's wired to seek out threats*, not because the prediction is anchored in evidence.

## Circles

TODO: Cleaner wording/grammar re: "tuning this stuff out..."
Part of tuning this stuff out comes from a lesson I first learned in [The 7 Habits of Highly Effective People](https://www.franklincovey.com/books/the-7-habits-of-highly-effective-people/) by Steven Covey. In Habit 1, Be Proactive, Covey introduces two concepts that are surprisingly relevant to how we engage with AI today: the Circle of Concern and the Circle of Influence.

> We each have a wide range of concerns, our health, our children, problems at work, the national debt, nuclear war… As we look at those things within our Circle of Concern, it becomes apparent that there are some things over which we have no real control and others that we can do something about. We could identify those concerns in the latter group by circumscribing them within a smaller Circle of Influence.

In other words, the Circle of Concern contains everything we care about, while the Circle of Influence contains the subset of concerns where our actions, habits, and decisions actually make a difference. Covey emphasizes that proactive people focus on the latter: they put energy into the things they can influence, expanding that circle over time. Reactive people, by contrast, spend most of their attention on worries outside their control, which can increase stress and shrink their influence.

Even though The 7 Habits of Highly Effective People was first published in 1989, this lesson is timeless. Applied to AI, much of the media hype, from doomsday scenarios to utopian promises, sits squarely in our Circle of Concern. Focusing on it might trigger anxiety or fascination, but it rarely changes anything we can act on.

By focusing instead on what we can influence, such as our skills, our understanding of AI tools, and how we use them in daily life, we can put energy into tangible progress. This mindset shift, from reactivity and speculation to acting within influence, is a simple but powerful way to regain control over our attention and mental bandwidth.

## What I Focus On

So how do I sift through the noise?

I've developed a simple personal filter:

**I tune in to content that's:**

* Educational or practical — e.g., how to use a new tool, how to solve a problem with AI in your workflow.
* Measured and specific — it gives tangible ideas instead of vague doom/utopia narratives.

**I skip content that's:**

* Entirely about "Will AI destroy everything?"
* Designed to provoke fear, outrage, or hype.
* Confidently predicting a distant future without *any clear basis in measurable reasoning*.

The result? I have more mental bandwidth to focus on what matters to me:

* learning the tools and integrating them into my daily work as an engineer,
* using AI to streamline problem‑solving,
* and even applying these tools to *everyday life* (like interpreting a medical lab results or summarizing scientific research).

That's my Circle of Influence, and it's where I believe attention, rather than fear is best spent.

## Closing Thoughts

AI is fascinating and important, and there will always be questions about where it's headed. But not all predictions are equally worth listening to. By understanding how our minds are wired, how predictions really stack up, and where our real influence lies, it's possible to cut through the hype and make your attention work for you, not against you.

## TODO
* From "predictions are wrong" to "negativity bias" question at end of one and another at beginning of next feels awkward
* intro para
  * awkward wording "it starts to feel like a constant signal drowned in noise"
* convert "What I Focus on" to more prose than bullet points maybe?
* Should the topic of superforecasters be a separate section?
  * are there any superforecasters in AI that we should be listening to?
  * is "Good Judgement Project" still active? Any AI analysis within that? https://en.wikipedia.org/wiki/The_Good_Judgment_Project
* edit
* add related after Jan. 1 published -> Slowing Down AI On Purpose
