---
title: "AI Forecasts, Fear, and Focus"
featuredImage: "../images/ai-forecasts-petr-sidorov-XoBdj1zV-EA-unsplash.jpg"
description: "Why I tune out sensational AI predictions and focus on what I can actually control."
date: "2026-01-15"
category: "productivity"
related:
  - "How to Learn New Things"
  - "The Machines Finally Made Us Care About Documentation"
  - "Rapid Prototyping with ChatGPT: OAS Pension Calculator Part 1"
---

Lately, I've noticed something odd about my podcast feed. An increasing number of episodes seems to have some flavor of "AI will do X or Y…" prediction — from the doom‑and‑gloom end ("AI will turn humans into paperclips!") to the utopian extreme ("AI will solve all our problems!"). And when you add in variants like "AI will take *all* our jobs by next year," it starts to feel like a constant signal drowned in noise.

Over time I've realized: this kind of content is low‑value, and I've consciously started tuning it out. Here's why.

## Most Predictions Are Wrong

There's a well‑known body of research showing that many confident expert predictions fail to come true. One of the most widely discussed studies on this was led by Philip E. Tetlock in his book *Expert Political Judgment* and later expanded in *Superforecasting*. Tetlock had hundreds of experts make tens of thousands of predictions about politics, economics, wars, and other complex domains — and then checked how those forecasts actually panned out.

The surprising result? Average experts were often no better at predicting the future than random chance, and the most confident pundits were often among the worst at forecasting. Credentials and media prominence didn't reliably translate into accuracy. ([Wikipedia][1])

What *did* matter was how people approached predictions:

* Those who used probabilistic thinking — assigning odds rather than absolutes — tended to do better.
* Those who updated their views with new evidence, instead of sticking to an initial bold claim, also fared better.
* A small subset of disciplined forecasters (often called "superforecasters") consistently outperformed average predictions over time, because they broke problems down and treated uncertainty explicitly. ([Richard Hughes-Jones][2])

This doesn't mean all prediction is useless — it means sensational, confident (and often vague) predictions are usually not trustworthy, especially when they're made for entertainment or clicks rather than clear, measurable foresight.

## Negativity Bias

Our brains pay more attention to fear...

Even when you know predictions aren't reliable, they're still everywhere. That's not an accident — it's rooted in how our brains are wired.

Psychologists call this the negativity bias — the tendency for negative information (threats, doom scenarios, alarming predictions) to grab and hold our attention more than positive or neutral information. This bias likely evolved because early humans needed to spot danger quickly to survive — missing a threat could mean death, whereas missing something positive (like berries) usually wasn't as costly. ([PubMed][3])

In our modern world, that ancient instinct gets hijacked by the attention economy:

* Fear and uncertainty sell.
* Negative headlines and predictions get clicks, listens, and engagement far more easily than calm, nuanced analysis.
* Doomscrolling (continually consuming bad news) is now a recognized phenomenon tied to our innate bias toward the negative. ([Wikipedia][4])

So when a podcast teases "Is AI going to end civilization?" or "Will AI take your job next quarter?", your brain pays attention *because it's wired to seek out threats*, not because the prediction is anchored in evidence.

## Circle of Concern vs. Circle of Influence

Part of why I tune this stuff out comes from something I first learned from Stephen Covey's "Circle of Concern" vs. "Circle of Influence." The idea — popularized in *The 7 Habits of Highly Effective People* — is simple but powerful: we all have a wide range of things we care about (our Circle of Concern), but only a subset of those things are within our power to *do something about* (our Circle of Influence). ([dplearningzone.the-dp.co.uk][5])

Worrying about how AI might affect the entire world twenty years from now is in the Circle of Concern — it might matter, but you and I don't have direct control over it. Spending energy there often leads to anxiety or distraction.

By contrast, focusing on what I *can* influence — like my own skills, learning, and how I use tools today — is in my Circle of Influence. Over time, that's where real progress happens. ([modern.works][6])

## What I Focus On

So how do I sift through the noise?

I've developed a simple personal filter:

**I tune in to content that's:**

* Educational or practical — e.g., how to use a new tool, how to solve a problem with AI in your workflow.
* Measured and specific — it gives tangible ideas instead of vague doom/utopia narratives.

**I skip content that's:**

* Entirely about "Will AI destroy everything?"
* Designed to provoke fear, outrage, or hype.
* Confidently predicting a distant future without *any clear basis in measurable reasoning*.

The result? I have more mental bandwidth to focus on what matters to me:

* learning the tools and integrating them into my daily work as an engineer,
* using AI to streamline problem‑solving,
* and even applying these tools to *everyday life* (like interpreting a medical lab results or summarizing scientific research).

That's my Circle of Influence, and it's where I believe attention, rather than fear is best spent.

## Closing Thoughts

AI is fascinating and important, and there will always be questions about where it's headed. But not all predictions are equally worth listening to. By understanding how our minds are wired, how predictions really stack up, and where our real influence lies, it's possible to cut through the hype and make your attention work for you, not against you.


[1]: https://en.wikipedia.org/wiki/Expert_Political_Judgment?utm_source=chatgpt.com "Expert Political Judgment"
[2]: https://www.richardhughesjones.com/superforecasting-summary/?utm_source=chatgpt.com "Superforecasting: The Art & Science of Prediction - Summary"
[3]: https://pubmed.ncbi.nlm.nih.gov/31750790/?utm_source=chatgpt.com "The negativity bias, revisited: Evidence from neuroscience measures and an individual differences approach - PubMed"
[4]: https://en.wikipedia.org/wiki/Doomscrolling?utm_source=chatgpt.com "Doomscrolling"
[5]: https://dplearningzone.the-dp.co.uk/wp-content/uploads/sites/2/2015/06/Covey.pdf?utm_source=chatgpt.com "[PDF] Stephen Covey's circle of concern and circle of influence"
[6]: https://www.modern.works/blog/the-power-of-coveys-circle-of-concern-influence-and-control?utm_source=chatgpt.com "The Power of Covey's Circle of Concern, Influence, and Control"


## TODO
* awkward wording "it starts to feel like a constant signal drowned in noise"
* more accurate lead up for negativity bias section
* work in references into inline markdown links
* superforecasters in AI?
* related - Slowing Down AI On Purpose
* intro para
* main content
* conclusion para
* edit
