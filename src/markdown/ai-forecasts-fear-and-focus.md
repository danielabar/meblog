---
title: "AI Forecasts, Fear, and Focus"
featuredImage: "../images/ai-forecasts-petr-sidorov-XoBdj1zV-EA-unsplash.jpg"
description: "Why I tune out sensational AI predictions and focus on what I can actually control."
date: "2026-01-15"
category: "productivity"
related:
  - "How to Learn New Things"
  - "The Machines Finally Made Us Care About Documentation"
  - "Rapid Prototyping with ChatGPT: OAS Pension Calculator Part 1"
---

Lately, I've noticed something disturbing about my podcast feed. An increasing number of episodes seems to have some flavor of "AI will do X or Y…" predictions. From the doom‑and‑gloom end ("AI will turn humans into paperclips!") to the utopian extreme ("AI will solve all our problems!"). And when you add in variants like "AI will take all our jobs by 2030", it starts to get very anxiety inducing.

Over time I've realized: this kind of content is low‑value, and I've consciously started tuning it out. Here's why.

## Most Predictions Are Wrong

There's a body of research showing that many confident expert predictions fail to come true. One of the most widely discussed studies on this was led by Philip E. Tetlock in his book [Expert Political Judgment](https://press.princeton.edu/books/hardcover/9780691178288/expert-political-judgment) and later expanded in [Superforecasting](https://www.penguinrandomhouse.com/books/227815/superforecasting-by-philip-e-tetlock-and-dan-gardner/). Tetlock had hundreds of experts make tens of thousands of predictions about politics, economics, wars, and other complex domains, and then checked how those forecasts actually panned out.

The surprising result? The majority of so-called "experts" were often no better at predicting the future than random chance, or a "dart-throwing chimpanzee" as Tetlock put it. The most confident pundits were often among the worst at forecasting. Credentials and media prominence didn't reliably translate into accuracy.

![dart throwing chimpanzee](../images/dart-throwing-chimpanzee.png "dart throwing chimpanzee")

What did matter was how people approached prediction itself. Tetlock found that a small group he called [superforecasters](https://www.richardhughesjones.com/superforecasting-summary/) consistently performed better by thinking in probabilities rather than absolutes, breaking complex questions into smaller parts, and continuously updating their beliefs as new evidence emerged. Their edge didn't come from bold confidence or impressive credentials, but from treating uncertainty as something to be measured and revised, not ignored.

This doesn't mean all predictions are useless. But it does mean sensational, confident (and often vague) predictions are usually not trustworthy, especially when they're made for capturing attention rather than clear, measurable foresight. Which raises a question: Is there any reason to believe today's AI doomer or utopian predictions are any more accurate than the forecasts Tetlock tested?

## Negativity Bias

WIP
Our brains pay more attention to fear...

TODO: Not accurate lead in
Even when you know predictions aren't reliable, they're still everywhere. That's not an accident — it's rooted in how our brains are wired.

Psychologists call this the negativity bias — the tendency for negative information (threats, doom scenarios, alarming predictions) to grab and hold our attention more than positive or neutral information. This bias likely evolved because early humans needed to spot danger quickly to survive — missing a threat could mean death, whereas missing something positive (like berries) usually wasn't as costly. ([PubMed][3])

In our modern world, that ancient instinct gets hijacked by the attention economy:

* Fear and uncertainty sell.
* Negative headlines and predictions get clicks, listens, and engagement far more easily than calm, nuanced analysis.
* Doomscrolling (continually consuming bad news) is now a recognized phenomenon tied to our innate bias toward the negative. ([Wikipedia][4])

So when a podcast teases "Is AI going to end civilization?" or "Will AI take your job next quarter?", your brain pays attention *because it's wired to seek out threats*, not because the prediction is anchored in evidence.

## Circles

Part of why I tune this stuff out comes from something I first learned from the book [The 7 Habits of Highly Effective People](https://www.franklincovey.com/books/the-7-habits-of-highly-effective-people/) by Steven Covey. In Habit 1, Be Proactive, Covey introduces two concepts that are surprisingly relevant to how we engage with AI today: the Circle of Concern and the Circle of Influence.

> We each have a wide range of concerns, our health, our children, problems at work, the national debt, nuclear war… As we look at those things within our Circle of Concern, it becomes apparent that there are some things over which we have no real control and others that we can do something about. We could identify those concerns in the latter group by circumscribing them within a smaller Circle of Influence.

In other words, the Circle of Concern contains everything we care about, while the Circle of Influence contains the subset of concerns where our actions, habits, and decisions actually make a difference. Covey emphasizes that proactive people focus on the latter: they put energy into the things they can influence, expanding that circle over time. Reactive people, by contrast, spend most of their attention on worries outside their control, which can increase stress and shrink their influence.

Even though The 7 Habits of Highly Effective People was first published in 1989, this lesson is timeless. Applied to AI, much of the media hype, from doomsday scenarios to utopian promises, sits squarely in our Circle of Concern. Focusing on it might trigger anxiety or fascination, but it rarely changes anything we can act on.

By focusing instead on what we can influence, such as our skills, our understanding of AI tools, and how we use them in daily life, we can put energy into tangible progress. This mindset shift, from reactivity and speculation to acting within influence, is a simple but powerful way to regain control over our attention and mental bandwidth.

## What I Focus On

So how do I sift through the noise?

I've developed a simple personal filter:

**I tune in to content that's:**

* Educational or practical — e.g., how to use a new tool, how to solve a problem with AI in your workflow.
* Measured and specific — it gives tangible ideas instead of vague doom/utopia narratives.

**I skip content that's:**

* Entirely about "Will AI destroy everything?"
* Designed to provoke fear, outrage, or hype.
* Confidently predicting a distant future without *any clear basis in measurable reasoning*.

The result? I have more mental bandwidth to focus on what matters to me:

* learning the tools and integrating them into my daily work as an engineer,
* using AI to streamline problem‑solving,
* and even applying these tools to *everyday life* (like interpreting a medical lab results or summarizing scientific research).

That's my Circle of Influence, and it's where I believe attention, rather than fear is best spent.

## Closing Thoughts

AI is fascinating and important, and there will always be questions about where it's headed. But not all predictions are equally worth listening to. By understanding how our minds are wired, how predictions really stack up, and where our real influence lies, it's possible to cut through the hype and make your attention work for you, not against you.


[3]: https://pubmed.ncbi.nlm.nih.gov/31750790/?utm_source=chatgpt.com "The negativity bias, revisited: Evidence from neuroscience measures and an individual differences approach - PubMed"
[4]: https://en.wikipedia.org/wiki/Doomscrolling?utm_source=chatgpt.com "Doomscrolling"
[5]: https://dplearningzone.the-dp.co.uk/wp-content/uploads/sites/2/2015/06/Covey.pdf?utm_source=chatgpt.com "[PDF] Stephen Covey's circle of concern and circle of influence"
[6]: https://www.modern.works/blog/the-power-of-coveys-circle-of-concern-influence-and-control?utm_source=chatgpt.com "The Power of Covey's Circle of Concern, Influence, and Control"


## TODO
* WIP work in references into inline markdown links and check them, update them if better resources available
* Stephen Covey's "circle" analogy - needs more work, more accuracy from original book (he never mentioned "circle of control" but rather, thinking of the things you can control as being within your circle of influence)
* intro para
  * awkward wording "it starts to feel like a constant signal drowned in noise"
* more accurate lead up for negativity bias section
* convert "What I Focus on" to more prose than bullet points maybe?
* Should the topic of superforecasters be a separate section?
  * are there any superforecasters in AI that we should be listening to?
  * is "Good Judgement Project" still active? Any AI analysis within that? https://en.wikipedia.org/wiki/The_Good_Judgment_Project
* edit
* add related after Jan. 1 published -> Slowing Down AI On Purpose
