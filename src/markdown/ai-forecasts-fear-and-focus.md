---
title: "AI Forecasts, Fear, and Focus"
featuredImage: "../images/ai-forecasts-petr-sidorov-XoBdj1zV-EA-unsplash.jpg"
description: "Why I tune out sensational AI predictions and focus on what I can actually control."
date: "2026-01-15"
category: "productivity"
related:
  - "How to Learn New Things"
  - "The Machines Finally Made Us Care About Documentation"
  - "Rapid Prototyping with ChatGPT: OAS Pension Calculator Part 1"
---

Lately, I've noticed something disturbing about my podcast feed. An increasing number of episodes seems to have some flavor of "AI will do X or Y…" predictions. From the doom‑and‑gloom end ("AI will turn humans into paperclips!") to the utopian extreme ("AI will solve all our problems!"). And when you add in variants like "AI will take all our jobs by 2030", it starts to get very anxiety inducing.

Over time I've realized: this kind of content is low‑value, and I've consciously started tuning it out. Here's why.

## Most Predictions Are Wrong

There's a body of research showing that many confident expert predictions fail to come true. One of the most widely discussed studies on this was led by Philip E. Tetlock in his book [Expert Political Judgment](https://press.princeton.edu/books/hardcover/9780691178288/expert-political-judgment) and later expanded in [Superforecasting](https://www.penguinrandomhouse.com/books/227815/superforecasting-by-philip-e-tetlock-and-dan-gardner/). Tetlock had hundreds of experts make tens of thousands of predictions about politics, economics, wars, and other complex domains, and then checked how those forecasts actually panned out.

The surprising result? The majority of so-called "experts" were often no better at predicting the future than random chance, or a "dart-throwing chimpanzee" as Tetlock put it. The most confident pundits were often among the worst at forecasting. Credentials and media prominence didn't reliably translate into accuracy.

![dart throwing chimpanzee](../images/dart-throwing-chimpanzee.png "dart throwing chimpanzee")

What did matter was how people approached prediction itself. Tetlock found that a small group he called [superforecasters](https://www.richardhughesjones.com/superforecasting-summary/) consistently performed better by thinking in probabilities rather than absolutes, breaking complex questions into smaller parts, and continuously updating their beliefs as new evidence emerged. Their edge didn't come from bold confidence or impressive credentials, but from treating uncertainty as something to be measured and revised, not ignored.

This doesn't mean all predictions are useless. But it does mean sensational, confident (and often vague) predictions are usually not trustworthy, especially when they're made for capturing attention rather than clear, measurable foresight. Which raises a question: Is there any reason to believe today's AI doomer or utopian predictions are any more accurate than the forecasts Tetlock tested?

## Negativity Bias

WIP
Our brains pay more attention to fear...

TODO: Not accurate lead in
Even when you know predictions aren't reliable, they're still everywhere. That's not an accident — it's rooted in how our brains are wired.

Psychologists call this the negativity bias — the tendency for negative information (threats, doom scenarios, alarming predictions) to grab and hold our attention more than positive or neutral information. This bias likely evolved because early humans needed to spot danger quickly to survive — missing a threat could mean death, whereas missing something positive (like berries) usually wasn't as costly. ([PubMed][3])

In our modern world, that ancient instinct gets hijacked by the attention economy:

* Fear and uncertainty sell.
* Negative headlines and predictions get clicks, listens, and engagement far more easily than calm, nuanced analysis.
* Doomscrolling (continually consuming bad news) is now a recognized phenomenon tied to our innate bias toward the negative. ([Wikipedia][4])

So when a podcast teases "Is AI going to end civilization?" or "Will AI take your job next quarter?", your brain pays attention *because it's wired to seek out threats*, not because the prediction is anchored in evidence.

## Circle of Concern vs. Circle of Influence

Part of why I tune this stuff out comes from something I first learned from Stephen Covey's "Circle of Concern" vs. "Circle of Influence." The idea — popularized in *The 7 Habits of Highly Effective People* — is simple but powerful: we all have a wide range of things we care about (our Circle of Concern), but only a subset of those things are within our power to *do something about* (our Circle of Influence). ([dplearningzone.the-dp.co.uk][5])

Worrying about how AI might affect the entire world twenty years from now is in the Circle of Concern — it might matter, but you and I don't have direct control over it. Spending energy there often leads to anxiety or distraction.

By contrast, focusing on what I *can* influence — like my own skills, learning, and how I use tools today — is in my Circle of Influence. Over time, that's where real progress happens. ([modern.works][6])

## What I Focus On

So how do I sift through the noise?

I've developed a simple personal filter:

**I tune in to content that's:**

* Educational or practical — e.g., how to use a new tool, how to solve a problem with AI in your workflow.
* Measured and specific — it gives tangible ideas instead of vague doom/utopia narratives.

**I skip content that's:**

* Entirely about "Will AI destroy everything?"
* Designed to provoke fear, outrage, or hype.
* Confidently predicting a distant future without *any clear basis in measurable reasoning*.

The result? I have more mental bandwidth to focus on what matters to me:

* learning the tools and integrating them into my daily work as an engineer,
* using AI to streamline problem‑solving,
* and even applying these tools to *everyday life* (like interpreting a medical lab results or summarizing scientific research).

That's my Circle of Influence, and it's where I believe attention, rather than fear is best spent.

## Closing Thoughts

AI is fascinating and important, and there will always be questions about where it's headed. But not all predictions are equally worth listening to. By understanding how our minds are wired, how predictions really stack up, and where our real influence lies, it's possible to cut through the hype and make your attention work for you, not against you.


[1]: https://en.wikipedia.org/wiki/Expert_Political_Judgment?utm_source=chatgpt.com "Expert Political Judgment"
[2]: https://www.richardhughesjones.com/superforecasting-summary/?utm_source=chatgpt.com "Superforecasting: The Art & Science of Prediction - Summary"
[3]: https://pubmed.ncbi.nlm.nih.gov/31750790/?utm_source=chatgpt.com "The negativity bias, revisited: Evidence from neuroscience measures and an individual differences approach - PubMed"
[4]: https://en.wikipedia.org/wiki/Doomscrolling?utm_source=chatgpt.com "Doomscrolling"
[5]: https://dplearningzone.the-dp.co.uk/wp-content/uploads/sites/2/2015/06/Covey.pdf?utm_source=chatgpt.com "[PDF] Stephen Covey's circle of concern and circle of influence"
[6]: https://www.modern.works/blog/the-power-of-coveys-circle-of-concern-influence-and-control?utm_source=chatgpt.com "The Power of Covey's Circle of Concern, Influence, and Control"


## TODO
* WIP work in references into inline markdown links and check them, update them if better resources available
* additional sentence at end of "most experts wrong" section, like is there any reason to suppose today's AI doomer or utopian predictors are any more accurate than a dart throwing chimpanzee?
* awkward wording "it starts to feel like a constant signal drowned in noise"
* more accurate lead up for negativity bias section
* are there any superforecasters in AI that we should be listening to?
* is "Good Judgement Project" still active? Any AI analysis within that?
* related - Slowing Down AI On Purpose
* intro para
* main content
* conclusion para
* edit
